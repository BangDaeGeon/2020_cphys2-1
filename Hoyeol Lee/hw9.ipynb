{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2016017032 물리학과 이호열\n",
    "## hw9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fashin MNIST\n",
    "## MNIST 패션 이미지를 CNN을 이용하여 분류하세요. CNN을 이용한 분류 결과를 MLP와 비교해보세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras import models \n",
    "from keras import layers\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.utils import to_categorical\n",
    "from keras.datasets import fashion_mnist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-Layer Perceptron (MLP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train, y_train), (X_test, y_test) = fashion_mnist.load_data()\n",
    "y_train=to_categorical(y_train)\n",
    "y_test=to_categorical(y_test)\n",
    "\n",
    "# preprocessing\n",
    "X_train = X_train.reshape((60000, 28*28))\n",
    "X_train = X_train/255\n",
    "\n",
    "X_test = X_test.reshape((10000, 28*28))\n",
    "X_test = X_test/255\n",
    "\n",
    "net = models.Sequential()\n",
    "net.add(layers.Dense(512, activation='relu', input_shape=(28*28,))) \n",
    "net.add(layers.Dense(512, activation='relu')) \n",
    "net.add(layers.Dropout(0.5))\n",
    "net.add(layers.Dense(512, activation='relu')) \n",
    "net.add(layers.Dropout(0.5))\n",
    "net.add(layers.Dense(10, activation='softmax')) \n",
    "net.compile(optimizer='nadam',loss='categorical_crossentropy',metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_val = X_train[:10000]\n",
    "partial_X_train = X_train[10000:]\n",
    "\n",
    "y_val = y_train[:10000]\n",
    "partial_y_train = y_train[10000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 50000 samples, validate on 10000 samples\n",
      "Epoch 1/10\n",
      "50000/50000 [==============================] - 15s 298us/step - loss: 0.5772 - acc: 0.7925 - val_loss: 0.4424 - val_acc: 0.8343\n",
      "Epoch 2/10\n",
      "50000/50000 [==============================] - 15s 292us/step - loss: 0.4189 - acc: 0.8525 - val_loss: 0.3810 - val_acc: 0.8611\n",
      "Epoch 3/10\n",
      "50000/50000 [==============================] - 14s 288us/step - loss: 0.3811 - acc: 0.8643 - val_loss: 0.3768 - val_acc: 0.8575\n",
      "Epoch 4/10\n",
      "50000/50000 [==============================] - 14s 289us/step - loss: 0.3631 - acc: 0.8718 - val_loss: 0.3687 - val_acc: 0.8660\n",
      "Epoch 5/10\n",
      "50000/50000 [==============================] - 15s 290us/step - loss: 0.3500 - acc: 0.8756 - val_loss: 0.3489 - val_acc: 0.8747\n",
      "Epoch 6/10\n",
      "50000/50000 [==============================] - 14s 289us/step - loss: 0.3382 - acc: 0.8800 - val_loss: 0.4126 - val_acc: 0.8542\n",
      "Epoch 7/10\n",
      "50000/50000 [==============================] - 15s 292us/step - loss: 0.3301 - acc: 0.8816 - val_loss: 0.3465 - val_acc: 0.8784\n",
      "Epoch 8/10\n",
      "50000/50000 [==============================] - 14s 289us/step - loss: 0.3214 - acc: 0.8856 - val_loss: 0.3594 - val_acc: 0.8705\n",
      "Epoch 9/10\n",
      "50000/50000 [==============================] - 14s 289us/step - loss: 0.3208 - acc: 0.8839 - val_loss: 0.3445 - val_acc: 0.8792\n",
      "Epoch 10/10\n",
      "50000/50000 [==============================] - 15s 292us/step - loss: 0.3149 - acc: 0.8882 - val_loss: 0.3751 - val_acc: 0.8689\n"
     ]
    }
   ],
   "source": [
    "val= net.fit(partial_X_train,partial_y_train,epochs=10, batch_size=64, validation_data=(X_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 1s 73us/step\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_acc = net.evaluate(X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 1s 79us/step\n",
      "\n",
      " Test accuracy: 0.863099992275238\n"
     ]
    }
   ],
   "source": [
    "score = net.evaluate(X_test, y_test)\n",
    "\n",
    "print('\\n', 'Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (60000, 28, 28) y_train shape: (60000,)\n"
     ]
    }
   ],
   "source": [
    "(X_train, y_train), (X_test, y_test) = fashion_mnist.load_data()\n",
    "\n",
    "print(\"X_train shape:\", X_train.shape, \"y_train shape:\", y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000 10000\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape[0], X_test.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.astype('float32') / 255\n",
    "X_test = X_test.astype('float32') / 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1d629e3f848>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 3 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.subplot(131)\n",
    "plt.imshow(X_train[0])\n",
    "plt.subplot(132)\n",
    "plt.imshow(X_train[1])\n",
    "plt.subplot(133)\n",
    "plt.imshow(X_train[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|#|category|\n",
    "|:---:|:---:|\n",
    "|0|T-shirt/top|\n",
    "|1|Trouser|\n",
    "|2|Pullover|\n",
    "|3|Dress|\n",
    "|4|Coat|\n",
    "|5|Sandal|\n",
    "|6|Shirt|\n",
    "|7|Sneaker|\n",
    "|8|Bag|\n",
    "|9|Ankle boot|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_val = X_train[:10000]\n",
    "partial_X_train = X_train[10000:]\n",
    "\n",
    "y_val = y_train[:10000]\n",
    "partial_y_train = y_train[10000:]\n",
    "\n",
    "X_train = X_train.reshape(X_train.shape[0], 28, 28, 1)\n",
    "X_val = X_val.reshape(X_val.shape[0], 28, 28, 1)\n",
    "X_test = X_test.reshape(X_test.shape[0], 28, 28, 1)\n",
    "\n",
    "y_train = to_categorical(y_train, 10)\n",
    "y_val = to_categorical(y_val, 10)\n",
    "y_test = to_categorical(y_test, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (60000, 28, 28, 1) y_train shape: (60000, 10)\n"
     ]
    }
   ],
   "source": [
    "print(\"X_train shape:\", X_train.shape, \"y_train shape:\", y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000 10000 10000\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape[0], X_val.shape[0], X_test.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.Sequential()\n",
    "\n",
    "model.add(layers.Conv2D(64, (2,2), padding='same', activation='relu', input_shape=(28,28,1))) \n",
    "model.add(layers.MaxPooling2D((2,2)))\n",
    "model.add(layers.Conv2D(64,(2,2), activation='relu'))\n",
    "\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(512, activation='relu')) \n",
    "model.add(layers.Dropout(0.5)) \n",
    "model.add(layers.Dense(64, activation='relu')) \n",
    "model.add(layers.Dropout(0.5)) \n",
    "model.add(layers.Dense(10, activation='softmax')) \n",
    "\n",
    "model.compile(optimizer='nadam',loss='categorical_crossentropy',metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/10\n",
      "60000/60000 [==============================] - 112s 2ms/step - loss: 0.5321 - acc: 0.8192 - val_loss: 0.3034 - val_acc: 0.8833\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.30344, saving model to model.weights.best.hdf5\n",
      "Epoch 2/10\n",
      "60000/60000 [==============================] - 111s 2ms/step - loss: 0.3353 - acc: 0.8839 - val_loss: 0.2272 - val_acc: 0.9149\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.30344 to 0.22725, saving model to model.weights.best.hdf5\n",
      "Epoch 3/10\n",
      "60000/60000 [==============================] - 112s 2ms/step - loss: 0.2782 - acc: 0.9027 - val_loss: 0.1667 - val_acc: 0.9378\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.22725 to 0.16670, saving model to model.weights.best.hdf5\n",
      "Epoch 4/10\n",
      "60000/60000 [==============================] - 111s 2ms/step - loss: 0.2460 - acc: 0.9125 - val_loss: 0.1530 - val_acc: 0.9440\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.16670 to 0.15298, saving model to model.weights.best.hdf5\n",
      "Epoch 5/10\n",
      "60000/60000 [==============================] - 112s 2ms/step - loss: 0.2133 - acc: 0.9234 - val_loss: 0.1277 - val_acc: 0.9512\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.15298 to 0.12774, saving model to model.weights.best.hdf5\n",
      "Epoch 6/10\n",
      "60000/60000 [==============================] - 113s 2ms/step - loss: 0.1918 - acc: 0.9329 - val_loss: 0.1238 - val_acc: 0.9583\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.12774 to 0.12382, saving model to model.weights.best.hdf5\n",
      "Epoch 7/10\n",
      "60000/60000 [==============================] - 113s 2ms/step - loss: 0.1727 - acc: 0.9377 - val_loss: 0.0981 - val_acc: 0.9641\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.12382 to 0.09810, saving model to model.weights.best.hdf5\n",
      "Epoch 8/10\n",
      "60000/60000 [==============================] - 113s 2ms/step - loss: 0.1594 - acc: 0.9439 - val_loss: 0.0802 - val_acc: 0.9716\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.09810 to 0.08024, saving model to model.weights.best.hdf5\n",
      "Epoch 9/10\n",
      "60000/60000 [==============================] - 112s 2ms/step - loss: 0.1436 - acc: 0.9486 - val_loss: 0.0694 - val_acc: 0.9755\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.08024 to 0.06942, saving model to model.weights.best.hdf5\n",
      "Epoch 10/10\n",
      "60000/60000 [==============================] - 113s 2ms/step - loss: 0.1303 - acc: 0.9548 - val_loss: 0.0559 - val_acc: 0.9798\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.06942 to 0.05590, saving model to model.weights.best.hdf5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x1d6308f4448>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "checkpointer = ModelCheckpoint(filepath='model.weights.best.hdf5', verbose = 1, save_best_only=True)\n",
    "\n",
    "model.fit(X_train, y_train, batch_size=64, epochs=10, validation_data=(X_val, y_val), callbacks=[checkpointer])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 3s 290us/step\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_acc = model.evaluate(X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Test accuracy: 0.9186999797821045\n"
     ]
    }
   ],
   "source": [
    "score = model.evaluate(X_test, y_test, verbose=0)\n",
    "\n",
    "print('\\n', 'Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MLP는 여러 문제를 풀기에 좋지만 영상이나 이미지를 처리하는데 문제가 있다.\n",
    "이미지 인식과 같은 분야에서 MLP를 사용하게 되면 MLP는 모든 입력이 위치와 상관없이 동일한 수준의 중요도를 갖는다고 보기 때문에 MLP를 이용해 fully-connected neural network를 구성하게 되면 파라미터의 크기가 엄청나게 커진다. \n",
    "\n",
    "반면에 CNN은 데이터의 특징을 추출하여 특징들의 패턴을 파악하는 구조이며  Convolution 과정과 Pooling 과정을 통해 진행되기에 CNN을 사용하게 된다면 파라미터의 크기를 MLP와 비교하여 엄청나게 줄일 수 있으며 이미지 전체에서 대표할 수 있는 불변하는 특징을 얻을 수 있기 때문에 이 특징을 학습시켜서 높은 인식 능력을 갖을 수 있다.\n",
    "\n",
    "위의 데이터의 결과로 볼 수 있듯이 MLP는 정확도가 86% 정도인 것에 반해 CNN으로 분류하였을 때는 90%가 넘는 정확도를 보여준다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "감점요소는 아니지만 partial_X_train 을 학습시켜야 하지 않나요"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
